# Transfer-Learning-based-sparse-reward-meta-Q-learning-Algorithm-for-Active-SLAM
Code for paper "Transfer Learning-based sparse-reward meta-Q-learning Algorithm for Active SLAM"
The code will be open-sourced after the paper is accepted.

# Abstract
Path planning is a widely researched topic, especially in complex unknown environments where dynamic changes and sensor drift introduce significant uncertainties. This paper proposes a transfer-learning-based prioritized experience replay meta-Q-learning algorithm. To solve the sparse reward problem caused by dynamic changes and sensor drift in complex unknown environments, a path planning algorithm based on sparse-reward meta-Q-learning is proposed. In the adaptive stage, the learned advantage function and transfer learning is introduced to improve the decision-making capability and generalisation of the algorithm, ensuring stable performance under uncertain conditions. To address the issues of low sample efficiency in the reinforcement learning experience replay buffer, sampling weights are designed, and bias estimation is used to maximise performance on new tasks in meta-learning. This approximated the policy to the optimal action decisions and further shortened the training time. Experimental results demonstrate that, compared to state-of-the-art meta-reinforcement learning algorithms, the proposed algorithm exhibits stronger robustness against uncertainties. It effectively avoids obstacles and successfully accomplishes localization and mapping tasks in dynamic and noisy environments, validating its reliability under uncertain conditions.
